Терминал – совокупность объектов инфраструктуры порта, технологически связанных между собой и предназначенных и (или) используемых для осуществления операций с грузами, в том числе для их перевалки, обслуживания судов, иных транспортных средств и (или) обслуживания. При оценке безопасности эксплуатации терминалов портов факторами опасности могут являться аварии перегрузочных машин и оборудования, судов, загрязнение окружающей среды, ухудшение здоровья, гибель человека или группы людей, материальный ущерб от произошедших опасностей.
Каждое нежелательное событие может возникать по отношению к определенному объекту риска, в связи с чем могут различаться технический, экологический, социальный и экономический риски, каждый из которых может иметь характерные источники и факторы риска.
Количественные характеристики для оценки риска возможно получить в результате оценки вероятности возникновения аварии либо расчетным путём обработки статистических данных об отказах, либо расчётным путём построения для отдельных узлов и деталей графиков, связывающих вероятность безотказной работы со временем эксплуатации.
Результатом этапа идентификации и декомпозиции рисков является реестр рисков перегрузочного терминала (таблица 2), где ключевыми моментами является подготовка мероприятий по реагированию на идентифицированные риски, в том числе включение в реестр специальных мероприятий для терминала, например, перевод единицы перегрузочного оборудования на другие операции, вместо проведения принятого внеочередного технического освидетельствования или обследования.
В случае, когда расчетные значения превышают допустимые, необходимо произвести структурные изменения дерева отказов. После изменений, например, при замене имеющихся составных элементов электрооборудования на элементы с более низкой интенсивностью отказов, либо при параллельном включении в схему дополнительных средств защиты, расчет проводится заново.
В общем случае задача поиска оптимального маршрута для произвольной геоинформационной системы представляется следующим образом: существует представление некой интересующей нас географической местности в определенной системе определений, и известны пункты отправления и назначения, описанные в этой же системе, необходимо найти такой путь (траекторию движения объекта в описанной системе) из пункта отправления до пункта назначения, который будет оптимальным по определенному ценовому критерию его прохождения из всех существующих в системе путей [14-18].
Тестирование модифицированной функции в качестве интегрального критерия цены пути алгоритма A^* показало, что предложенная модификация обладает той же вычислительной сложностью, что и оригинальный алгоритм (полиномиальная сложность), и в условиях тестовой сети успешно находит оптимальный путь.
В статье «Разработка частотно-позиционного метода анализа текстов на консонантно-вокалических языках» данной конференции было приведено теоретическое описание частотно-позиционного метода анализа текстов, основанного на гипотезе, что часто встречающиеся слова, за исключением стоп-слов, обладают большим семантическим весом, чем редко встречающиеся слова. Основная описанного метода состоит в том, что, используя частоту вхождения слова в синтаксические конструкции (предложение, абзац, текст), позицию слова в тексте, а также применение различных эвристик по определению семантических весовых коэффициентов для элементов текста, можно определить основные дескрипторные конструкции (ключевые слова, словосочетания, предложения) и описать семантический «скелет» текста, который можно использовать, например, в задачах классификации [9-16].
Перед тем как приступать к анализу текстов, необходимо произвести нормализацию исходных текстов – убрать лишние символы: лишние пробелы, табуляции, переносы строк и т.д., проверить количество абзацев – оно должно совпадать с исходным текстом, необходимо убрать предлоги, служебные конструкции и слова-филлеры – все стоп-слова, чтобы они не «загрязняли» частотный спектр текста, т.к. они очень часто фигурируют в синтаксических структурах. Также необходимо отформатировать сами слова – провести стемминг слов: убрать флективные части – окончания, приставки, т. к. при лексикографическом сравнении слов (литеральных строк), слова «информация» и «информацию» будут не равны, хотя имеют один и тот же смысл.
Второй эксперимент заключается в тестировании метода для решения задач сравнения текстов как по семантическим, так и по лексическим критериям. Для этого 15 членов тестовой группы, состоящих преимущественно из студентов технического университета, не знающих о сути данной работы, написали изложение по выбранному эталонному тексту на тему «Информационные процессы» объемом в 458 слов. Текст читался три раза и по правилам чтения изложения. Таким образом, были получены 15 текстов изложений, которые были проверены тремя экспертами.
В лингвистике приоритетным направлением является изучение проблемы понимания текста, т. к. динамичный ритм развития общества и технологий влечет за собой ежегодное увеличение объема информации, который человек должен успевать максимально быстро получать и понимать. На данном этапе практически каждый индивид сталкивается все чаще с тем, что смысл прочитанного текста искажается, а зачастую из текста выносится отнюдь не главная мысль автора(ов) [1-8]. 
Для более удобного анализа в качестве исследуемого информационного объекта выберем текст, тогда семантическими элементами будут выступать предложения, словосочетания, слова и основы слов, несущие определенный смысловой контекст.
Несмотря на, казалось бы, большую приближенность метода и определенные недостатки, такие как нечувствительность к контексту и неточность семантического анализа, у данного метода есть существенные плюсы:
	во-первых, как уже говорилось выше, данный метод обладает точно определенными параметрами (частота и позиция), которые легко обрабатывать автоматически;
	во-вторых, абсолютная независимость от тематики текста – программе не обязательно знать какая тематика и какой смысл у эталонного текста, что делает данный метод применимый к различным текстам.
Для нейросетевого же метода мало того, что необходимо знать тематику текста, нужно еще создавать под данную тематику нейронную сеть и обучать ее, причем, чем точнее мы хотим получить результат, тем больше обучающих текстов нужно иметь, чтобы обучить сеть.
Перед тем, как осуществлять анализ текстов, как правило, текст подвергают процедуре нормализации, которая зависит от типа языка. Обычно к процедурам нормализации относятся: удаление междометий, слов-филлеров, служебных конструкций (для синтетических языков) и т.д. Также при обработке текстов на флективных языках может производится нормализация слов, которая заключается в удалении флективных частей слов – окончаний, суффиксов и т.д., чтобы обеспечить корректное сравнение слов.
Для примера, в качестве синтаксического объекта возьмем предложение, семантическим элементом тогда будет являться слово или его нормализованная версия, а в качестве рассматриваемой части текста выберем абзац, так как и предложения, и абзац являются формами завершенной мысли.
Тогда на основе гипотезы изложенной выше можно выдвинуть следующее предположение: предложение, содержащие слова, которые наиболее часто встречаются в абзаце, несет основную смысловую нагрузку данного абзаца.
